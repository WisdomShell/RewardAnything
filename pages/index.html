---
layout: default
title: "Home"
description: "RewardAnything: Generalizable Principle-Following Reward Models"
---

<!-- Hero Section -->
<section class="gradient-bg text-slate-700 py-12">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <div class="mb-6">
            <!-- Use your regular logo - it will look perfect on this lighter background -->
            <img src="{{ '/assets/images/rewardanything-logo-horizontal.png' | relative_url }}" 
                 alt="RewardAnything" 
                 class="h-12 md:h-16 w-auto mx-auto mb-4">
        </div>
        
        <p class="text-xl md:text-2xl mb-6 text-slate-700 max-w-4xl mx-auto font-bold">
            Generalizable Principle-Following Reward Models
        </p>
        
        <!-- Enhanced TLDR with better mobile typography -->
        <div class="text-sm md:text-base mb-8 text-slate-600 max-w-5xl mx-auto leading-relaxed">
            <p class="mb-3 text-xs md:text-sm lg:text-base">
                Traditional reward models learn <span class="font-semibold text-slate-800">implicit preferences</span> behind chosen-rejected pairs,<br class="hidden sm:block">
                but human values are <span class="italic text-slate-700">far more nuanced</span> than any single distribution.
            </p>
            <p class="text-xs md:text-sm lg:text-base">
                <span class="font-semibold text-slate-800">Just as LLMs follow instructions</span>, reward models should follow<br class="hidden sm:block">
                <span class="font-semibold text-blue-700">explicitly specified principles</span>‚Äîenabling<br class="hidden md:block"> 
                <span class="underline decoration-2 decoration-blue-600">dynamic adaptation</span> to diverse evaluation criteria<br class="hidden sm:block">
                <span class="font-medium text-slate-700">without costly retraining</span>.
            </p>
        </div>
        
        <!-- Updated CTA Buttons -->
        <div class="flex flex-col sm:flex-row gap-3 justify-center items-center mb-8">
            <a href="{{ site.paper_url | default: '#' }}" 
               class="inline-flex items-center bg-white text-slate-700 px-6 py-2.5 rounded-lg font-semibold hover:bg-slate-50 transition-all transform hover:scale-105 shadow-lg border border-slate-200">
                üìÑ <span class="ml-2">Paper</span>
            </a>
            <a href="{{ site.huggingface_url | default: '#' }}" 
               class="inline-flex items-center bg-gradient-to-r from-yellow-400 to-orange-500 text-white px-6 py-2.5 rounded-lg font-semibold hover:from-yellow-500 hover:to-orange-600 transition-all transform hover:scale-105 shadow-lg">
                ü§ó <span class="ml-2">Model Weights</span>
            </a>
            <a href="#quickstart" 
               class="inline-flex items-center bg-gradient-to-r from-emerald-500 to-teal-600 text-white px-6 py-2.5 rounded-lg font-semibold hover:from-emerald-600 hover:to-teal-700 transition-all transform hover:scale-105 shadow-lg">
                üíª <span class="ml-2">Get Started</span>
            </a>
        </div>
    </div>
</section>

<!-- Authors Section -->
<section class="py-8 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 text-center">
        <div class="text-base md:text-lg text-gray-700 mb-4">
            <div class="space-y-2">
                <!-- First line of authors (5 authors) -->
                <div class="flex flex-wrap justify-center gap-x-6 gap-y-2">
                    <span>Zhuohao Yu<sup>1,¬ß</sup></span>
                    <span>Jiali Zeng<sup>2</sup></span>
                    <span>Weizheng Gu<sup>1</sup></span>
                    <span>Yidong Wang<sup>1</sup></span>
                    <span>Jindong Wang<sup>3</sup></span>
                </div>
                <!-- Second line of authors (5 authors) -->
                <div class="flex flex-wrap justify-center gap-x-6 gap-y-2">
                    <span>Fandong Meng<sup>2</sup></span>
                    <span>Jie Zhou<sup>2</sup></span>
                    <span>Yue Zhang<sup>4</sup></span>
                    <span>Shikun Zhang<sup>1</sup></span>
                    <span>Wei Ye<sup>1,‚Ä†</sup></span>
                </div>
            </div>
        </div>
        <div class="text-base md:text-lg text-gray-600">
            <div class="mb-3">
                <sup>1</sup>Peking University &emsp;
                <sup>2</sup>WeChat AI &emsp;
                <sup>3</sup>William & Mary &emsp;
                <sup>4</sup>Westlake University
            </div>
            <p><sup>¬ß</sup>Work done during internship at WeChat AI &emsp; <sup>‚Ä†</sup>Corresponding author</p>
        </div>
    </div>
</section>

<!-- Problem & Solution Overview -->
<section class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">The Problem with Current Reward Models</h2>
            <p class="text-xl text-gray-600 max-w-4xl mx-auto">
                Traditional reward models learn implicit preferences from fixed datasets, making them rigid and unable to adapt to diverse real-world needs.
            </p>
        </div>

        <!-- Research Figure Container -->
        <div class="paper-figure-container mb-16">
            <div class="max-w-5xl mx-auto">
                <img src="{{ '/assets/images/figure_1_placeholder.jpg' | relative_url }}" 
                     alt="Figure 1: Current post-training optimization paradigm vs RewardAnything approach"
                     class="w-full h-auto rounded-lg shadow-sm">
                <p class="text-sm text-gray-600 text-center mt-4 italic">
                    <strong>Figure 1:</strong> Current post-training optimization requires costly retraining for different preferences. 
                    RewardAnything directly follows natural language principles without retraining.
                </p>
            </div>
        </div>

        <div class="grid grid-cols-1 lg:grid-cols-2 gap-12">
            <!-- Current Limitations -->
            <div class="bg-red-50 p-8 rounded-xl">
                <div class="text-red-600 text-2xl mb-4">‚ö†Ô∏è</div>
                <h3 class="text-xl font-bold text-red-900 mb-4">Current Limitations</h3>
                <ul class="space-y-3 text-red-800">
                    <li class="flex items-start">
                        <span class="text-red-600 mr-2">‚Ä¢</span>
                        <span><strong>Limited Adaptability:</strong> Need to retrain for different preference criteria</span>
                    </li>
                    <li class="flex items-start">
                        <span class="text-red-600 mr-2">‚Ä¢</span>
                        <span><strong>Implicit Learning:</strong> Learn biases from spurious correlations in data</span>
                    </li>
                    <li class="flex items-start">
                        <span class="text-red-600 mr-2">‚Ä¢</span>
                        <span><strong>Costly Updates:</strong> Require new preference data collection and retraining</span>
                    </li>
                    <li class="flex items-start">
                        <span class="text-red-600 mr-2">‚Ä¢</span>
                        <span><strong>Poor Interpretability:</strong> Difficult to understand decision rationale</span>
                    </li>
                </ul>
            </div>

            <!-- Our Solution -->
            <div class="bg-green-50 p-8 rounded-xl">
                <div class="text-green-600 text-2xl mb-4">‚úÖ</div>
                <h3 class="text-xl font-bold text-green-900 mb-4">RewardAnything Solution</h3>
                <ul class="space-y-3 text-green-800">
                    <li class="flex items-start">
                        <span class="text-green-600 mr-2">‚Ä¢</span>
                        <span><strong>Principle-Following:</strong> Adapt to any explicit natural language criteria</span>
                    </li>
                    <li class="flex items-start">
                        <span class="text-green-600 mr-2">‚Ä¢</span>
                        <span><strong>Zero Retraining:</strong> Dynamic adaptation without model updates</span>
                    </li>
                    <li class="flex items-start">
                        <span class="text-green-600 mr-2">‚Ä¢</span>
                        <span><strong>Transparent Reasoning:</strong> Explicit explanations for decisions</span>
                    </li>
                    <li class="flex items-start">
                        <span class="text-green-600 mr-2">‚Ä¢</span>
                        <span><strong>Bias Mitigation:</strong> Clear principles eliminate spurious correlations</span>
                    </li>
                </ul>
            </div>
        </div>
    </div>
</section>

<!-- Quick Start -->
<section id="quickstart" class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">Quick Start</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                Get started with RewardAnything in just a few lines of code
            </p>
        </div>

        <div class="max-w-4xl mx-auto">
            <!-- Installation -->
            <div class="bg-gray-50 p-6 rounded-xl mb-8">
                <h3 class="text-lg font-semibold text-gray-900 mb-4">Installation</h3>
                <div class="bg-gray-900 text-green-400 p-4 rounded-lg font-mono text-sm">
                    <span class="text-gray-500"># Install from PyPI</span><br>
                    pip install reward-anything
                </div>
            </div>

            <!-- Basic Usage -->
            <div class="bg-gray-50 p-6 rounded-xl mb-8">
                <h3 class="text-lg font-semibold text-gray-900 mb-4">Basic Usage</h3>
                <div class="bg-gray-900 text-white p-4 rounded-lg font-mono text-sm overflow-x-auto">
<pre><code><span class="text-blue-400">from</span> reward_anything <span class="text-blue-400">import</span> RewardModel

<span class="text-gray-500"># Initialize the model</span>
rm = RewardModel.from_pretrained(<span class="text-green-400">"reward-anything-8b"</span>)

<span class="text-gray-500"># Define your principle</span>
principle = <span class="text-green-400">"Prioritize responses that are helpful, accurate, and concise."</span>

<span class="text-gray-500"># Evaluate responses</span>
prompt = <span class="text-green-400">"How do I learn machine learning?"</span>
responses = [<span class="text-green-400">"Read papers and practice coding"</span>, <span class="text-green-400">"Just use ChatGPT"</span>]

result = rm.evaluate(principle, prompt, responses)
<span class="text-blue-400">print</span>(result.scores)  <span class="text-gray-500"># [4.2, 2.1]</span>
<span class="text-blue-400">print</span>(result.ranking) <span class="text-gray-500"># ['response_1', 'response_2']</span></code></pre>
                </div>
            </div>

            <!-- Advanced Example -->
            <div class="bg-blue-50 p-6 rounded-xl">
                <h3 class="text-lg font-semibold text-blue-900 mb-4">Advanced: Custom Principles</h3>
                <div class="bg-gray-900 text-white p-4 rounded-lg font-mono text-sm overflow-x-auto">
<pre><code><span class="text-gray-500"># Use complex, task-specific principles</span>
safety_principle = <span class="text-green-400">"""
Safety is paramount. Responses should:
1. Refuse harmful requests with empathy
2. Provide constructive alternatives
3. Maintain helpful tone without being preachy
"""</span>

<span class="text-gray-500"># Works for any domain without retraining</span>
code_principle = <span class="text-green-400">"Prefer clean, readable code with good documentation"</span>
creative_principle = <span class="text-green-400">"Value originality and emotional resonance"</span></code></pre>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Performance Results -->
<section class="py-20 bg-gradient-to-br from-blue-50 to-indigo-100">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">State-of-the-Art Performance</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                RewardAnything achieves excellent performance on both traditional benchmarks and our new principle-following evaluation
            </p>
        </div>

        <div class="grid grid-cols-1 md:grid-cols-3 gap-8">
            <!-- RM-Bench Results -->
            <div class="bg-white p-8 rounded-xl shadow-sm">
                <div class="text-center mb-6">
                    <div class="text-3xl font-bold text-blue-600 mb-2">89.2%</div>
                    <div class="text-lg font-semibold text-gray-900">RM-Bench Accuracy</div>
                    <div class="text-sm text-gray-600">State-of-the-art on challenging "hard" settings</div>
                </div>
                <div class="space-y-3 text-sm">
                    <div class="flex justify-between">
                        <span class="text-gray-600">Math:</span>
                        <span class="font-semibold">89.1%</span>
                    </div>
                    <div class="flex justify-between">
                        <span class="text-gray-600">Code:</span>
                        <span class="font-semibold">89.1%</span>
                    </div>
                    <div class="flex justify-between">
                        <span class="text-gray-600">Safety:</span>
                        <span class="font-semibold">89.6%</span>
                    </div>
                </div>
            </div>

            <!-- RABench Results -->
            <div class="bg-white p-8 rounded-xl shadow-sm">
                <div class="text-center mb-6">
                    <div class="text-3xl font-bold text-green-600 mb-2">80.5%</div>
                    <div class="text-lg font-semibold text-gray-900">RABench Accuracy</div>
                    <div class="text-sm text-gray-600">Novel benchmark for principle-following</div>
                </div>
                <div class="space-y-3 text-sm">
                    <div class="flex justify-between">
                        <span class="text-gray-600">Content:</span>
                        <span class="font-semibold">82.6%</span>
                    </div>
                    <div class="flex justify-between">
                        <span class="text-gray-600">Logic:</span>
                        <span class="font-semibold">80.7%</span>
                    </div>
                    <div class="flex justify-between">
                        <span class="text-gray-600">Style:</span>
                        <span class="font-semibold">80.5%</span>
                    </div>
                </div>
            </div>

            <!-- Key Advantages -->
            <div class="bg-white p-8 rounded-xl shadow-sm">
                <div class="text-center mb-6">
                    <div class="text-3xl font-bold text-purple-600 mb-2">Zero</div>
                    <div class="text-lg font-semibold text-gray-900">Retraining Required</div>
                    <div class="text-sm text-gray-600">Adapt to new principles instantly</div>
                </div>
                <div class="space-y-3 text-sm">
                    <div class="flex items-center">
                        <div class="w-2 h-2 bg-green-500 rounded-full mr-2"></div>
                        <span>200+ principle categories</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-2 h-2 bg-green-500 rounded-full mr-2"></div>
                        <span>Listwise evaluation</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-2 h-2 bg-green-500 rounded-full mr-2"></div>
                        <span>Transparent reasoning</span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Key Features -->
<section class="py-20 bg-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">Key Innovations</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                RewardAnything introduces novel techniques for principle-following reward modeling
            </p>
        </div>

        <div class="grid grid-cols-1 md:grid-cols-2 gap-12">
            <div class="space-y-8">
                <div class="space-y-4">
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Group Relative Policy Optimization (GRPO)</h4>
                            <p class="text-gray-600">Advanced RL training that learns relative preferences within response groups</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Listwise Evaluation</h4>
                            <p class="text-gray-600">Efficient ranking of multiple responses in a single forward pass</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-blue-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Inference-Time Reasoning</h4>
                            <p class="text-gray-600">Explicit reasoning process for transparent decision making</p>
                        </div>
                    </div>
                </div>
                <div class="space-y-4">
                    <div class="flex items-start">
                        <div class="bg-green-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Multi-LLM Consensus</h4>
                            <p class="text-gray-600">Ground truth from 4 state-of-the-art LLMs with algorithmic consensus</p>
                        </div>
                    </div>
                    <div class="flex items-start">
                        <div class="bg-green-100 p-2 rounded-lg mr-4">
                            <svg class="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 20 20">
                                <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
                            </svg>
                        </div>
                        <div>
                            <h4 class="font-semibold text-gray-900">Human Verification</h4>
                            <p class="text-gray-600">89% agreement rate with Œ∫=0.57 for reliable evaluation standards</p>
                        </div>
                    </div>
                </div>
            </div>

            <!-- RABench Description -->
            <div class="bg-gray-50 p-8 rounded-xl">
                <h3 class="text-xl font-bold text-gray-900 mb-4">RABench: Novel Evaluation Framework</h3>
                <p class="text-gray-700 mb-6">
                    We introduce RABench, a comprehensive benchmark specifically designed to evaluate reward models' 
                    ability to follow explicit natural language principles across diverse domains and criteria.
                </p>
                <div class="space-y-4">
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-blue-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>1,002 validated rankings</strong> across 50 principles</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-green-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>5 principle categories:</strong> Content, Logic, Style, Tone, Structure</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-purple-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>Multiple domains:</strong> Chat, Code, Safety, Math</span>
                    </div>
                    <div class="flex items-center">
                        <div class="w-3 h-3 bg-orange-500 rounded-full mr-3"></div>
                        <span class="text-gray-700"><strong>Human-verified quality</strong> with high inter-annotator agreement</span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Documentation -->
<section id="documentation" class="py-20 bg-gray-50">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-16">
            <h2 class="text-3xl md:text-4xl font-bold text-gray-900 mb-4">Documentation & Resources</h2>
            <p class="text-xl text-gray-600 max-w-3xl mx-auto">
                Everything you need to understand and use RewardAnything for your research and applications
            </p>
        </div>

        <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4 gap-6">
            <a href="{{ site.paper_url | default: '#' }}" class="block bg-blue-50 p-6 rounded-xl hover:bg-blue-100 transition-colors">
                <div class="text-2xl mb-3">üìÑ</div>
                <h3 class="font-semibold text-blue-900 mb-2">Research Paper</h3>
                <p class="text-sm text-blue-700">Complete methodology, experiments, and theoretical foundations</p>
            </a>

            <a href="#" class="block bg-green-50 p-6 rounded-xl hover:bg-green-100 transition-colors">
                <div class="text-2xl mb-3">üöÄ</div>
                <h3 class="font-semibold text-green-900 mb-2">API Documentation</h3>
                <p class="text-sm text-green-700">Comprehensive guide to using RewardAnything in your code</p>
            </a>

            <a href="#" class="block bg-purple-50 p-6 rounded-xl hover:bg-purple-100 transition-colors">
                <div class="text-2xl mb-3">üìä</div>
                <h3 class="font-semibold text-purple-900 mb-2">RABench Dataset</h3>
                <p class="text-sm text-purple-700">Benchmark dataset for evaluating principle-following capabilities</p>
            </a>

            <a href="{{ site.huggingface_url | default: '#' }}" class="block bg-orange-50 p-6 rounded-xl hover:bg-orange-100 transition-colors">
                <div class="text-2xl mb-3">ü§ó</div>
                <h3 class="font-semibold text-orange-900 mb-2">Model Weights</h3>
                <p class="text-sm text-orange-700">Pre-trained models ready for inference and fine-tuning</p>
            </a>
        </div>
    </div>
</section>

<!-- Citation -->
<section class="py-20 bg-gray-900 text-white">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
        <div class="text-center mb-12">
            <h2 class="text-3xl md:text-4xl font-bold mb-4">Citation</h2>
            <p class="text-xl text-gray-300 max-w-3xl mx-auto">
                If you use RewardAnything in your research, please cite our paper
            </p>
        </div>

        <div class="max-w-4xl mx-auto">
            <div class="bg-gray-800 p-6 rounded-xl">
                <pre class="text-gray-300 font-mono text-sm overflow-x-auto"><code>@article{yu2024rewardanything,
  title={RewardAnything: Generalizable Principle-Following Reward Models},
  author={Yu, Zhuohao and Zeng, Jiali and Gu, Weizheng and Wang, Yidong and 
          Wang, Jindong and Meng, Fandong and Zhou, Jie and Zhang, Yue and 
          Zhang, Shikun and Ye, Wei},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}</code></pre>
            </div>
        </div>
    </div>
</section>

<style>
/* Special container for paper figures with neutral background */
.paper-figure-container {
    background: linear-gradient(135deg, #f8fafc 0%, #f1f5f9 100%);
    padding: 2rem;
    border-radius: 1rem;
    border: 1px solid #e2e8f0;
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05);
}

.paper-figure-container img {
    background: white;
    padding: 1rem;
    border-radius: 0.5rem;
    box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}
</style> 